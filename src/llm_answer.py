"""
LLM answer generation for the maintenance manual RAG app.

This module is responsible for:
-Taking a user query + retrieval manual chunks (from ManualRAG)
-Building a prompt that constrains the LLM to use only that context
-Calling the OpenAI Chat Completions API (gpt-4.1-mini)
-Returning a natural language answer

This section DOES NOT handle retrieval or PDF parsing; that is all
contained within the src/rag_manual.py
"""
from typing import List

from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables (OPENAI_API_KEY)
load_dotenv()
#Create a single OpenAI client instance. 
#The client automatically picks up OPEN_API_KEY from the environment.
client = OpenAI()


def answer_from_chunks(query: str, chunks: List[str]) -> str:
    """
    Generate an answer to 'query' using the provided context 'chunks'.

    Steps:
    1. Combine the retrieved chunks into a single "context" string
    2. Build a system message that explains the assistants role and constraints.
    3. Build a user message that includes both the context and the actual question.
    4. Call the OpenAI Chat Completions API (gpt-4.1-mini).
    5. Return the model's answer text.
    
    :param query: The user's natural-language question
    :param chunks: A list of text chunks retrieved from the manual by ManualRAG
    :return: The answer text generated by the LLM
    :raises RuntimeError: For known issues like hitting API quota limits
    """
    context = "\n\n---\n\n".join(chunks)
    #System message defines the assistants behavior and boundries
    system_msg = (
        "You are a helpful maintenance assistant. "
        "Only answer using the information in the provided manual context. "
        "If the answer cannot be found, say you don't know."
    )
    #User message includes both the context and the explicit question.
    #The prompt asks the model to give a clear, step-by-step answer.
    user_msg = (
        f"Manual context:\n{context}\n\n"
        f"User question: {query}\n\n"
        "Answer in a clear, step-by-step way."
    )
    #Call  the OpenAI Chat Completions API.
    #gpt-4.1-mini is a good balance of cost and capability for this use case
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
    )
    #Extract the text of the first choice in the response. 
    return response.choices[0].message.content